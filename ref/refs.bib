@article{DBLP:journals/corr/abs-1906-05264,
  author    = {Alexander Alexandrov and
               Konstantinos Benidis and
               Michael Bohlke{-}Schneider and
               Valentin Flunkert and
               Jan Gasthaus and
               Tim Januschowski and
               Danielle C. Maddix and
               Syama Sundar Rangapuram and
               David Salinas and
               Jasper Schulz and
               Lorenzo Stella and
               Ali Caner T{\"{u}}rkmen and
               Yuyang Wang},
  title     = {GluonTS: Probabilistic Time Series Models in Python},
  journal   = {CoRR},
  volume    = {abs/1906.05264},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05264},
  archivePrefix = {arXiv},
  eprint    = {1906.05264},
  timestamp = {Mon, 17 Jun 2019 15:05:27 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-05264.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{siegel2013predictive,
  title={Predictive analytics: The power to predict who will click, buy, lie, or die},
  author={Siegel, Eric},
  volume={10},
  year={2013},
  publisher={Wiley Hoboken}
}

@article{lebedev2018speeding,
  title={Speeding-up convolutional neural networks: A survey},
  author={Lebedev, Vadim and Lempitsky, Victor},
  journal={Bulletin of the Polish Academy of Sciences. Technical Sciences},
  volume={66},
  number={6},
  year={2018}
}

@article{MAKRIDAKIS202054,
title = {The M4 Competition: 100,000 time series and 61 forecasting methods},
journal = {International Journal of Forecasting},
volume = {36},
number = {1},
pages = {54-74},
year = {2020},
note = {M4 Competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301128},
author = {Spyros Makridakis and Evangelos Spiliotis and Vassilios Assimakopoulos},
keywords = {Forecasting competitions, M competitions, Forecasting accuracy, Prediction intervals, Time series methods, Machine learning methods, Benchmarking methods, Practice of forecasting},
abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.}
}

@inproceedings{dabrowski2020forecastnet,
  title={ForecastNet: A Time-Variant Deep Feed-Forward Neural Network Architecture for Multi-step-Ahead Time-Series Forecasting},
  author={Dabrowski, Joel Janek and Zhang, YiFan and Rahman, Ashfaqur},
  booktitle={International Conference on Neural Information Processing},
  pages={579--591},
  year={2020},
  organization={Springer}
}

@article{smyl2020hybrid,
  title={A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting},
  author={Smyl, Slawek},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={75--85},
  year={2020},
  publisher={Elsevier}
}

@article{salinas2020deepar,
  title={DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
  author={Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  journal={International Journal of Forecasting},
  volume={36},
  number={3},
  pages={1181--1191},
  year={2020},
  publisher={Elsevier}
}

@article{hewamalage2021recurrent,
  title={Recurrent neural networks for time series forecasting: Current status and future directions},
  author={Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
  journal={International Journal of Forecasting},
  volume={37},
  number={1},
  pages={388--427},
  year={2021},
  publisher={Elsevier}
}

@article{de2019gru,
  title={Gru-ode-bayes: Continuous modeling of sporadically-observed time series},
  author={De Brouwer, Edward and Simm, Jaak and Arany, Adam and Moreau, Yves},
  journal={arXiv preprint arXiv:1905.12374},
  year={2019}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@book{amari2003handbook,
  title={The handbook of brain theory and neural networks},
  author={Amari, Shunʼichi and others},
  year={2003},
  publisher={MIT press}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@inproceedings{binkowski2018autoregressive,
  title={Autoregressive convolutional neural networks for asynchronous time series},
  author={Binkowski, Mikolaj and Marti, Gautier and Donnat, Philippe},
  booktitle={International Conference on Machine Learning},
  pages={580--589},
  year={2018},
  organization={PMLR}
}

@article{shi2015convolutional,
  title={Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  author={Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  journal={arXiv preprint arXiv:1506.04214},
  year={2015}
}

@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{pascanu2013construct,
  title={How to construct deep recurrent neural networks},
  author={Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6026},
  year={2013}
}

@article{bordes2005fast,
  title={Fast kernel classifiers with online and active learning.},
  author={Bordes, Antoine and Ertekin, Seyda and Weston, Jason and Botton, L{\'e}on and Cristianini, Nello},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={9},
  year={2005}
}

@inproceedings{zhao2015stochastic,
  title={Stochastic optimization with importance sampling for regularized loss minimization},
  author={Zhao, Peilin and Zhang, Tong},
  booktitle={international conference on machine learning},
  pages={1--9},
  year={2015},
  organization={PMLR}
}

@inproceedings{wu2017sampling,
  title={Sampling matters in deep embedding learning},
  author={Wu, Chao-Yuan and Manmatha, R and Smola, Alexander J and Krahenbuhl, Philipp},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2840--2848},
  year={2017}
}

@article{needell2014stochastic,
  title={Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm},
  author={Needell, Deanna and Ward, Rachel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={1017--1025},
  year={2014}
}

@article{fan2016neural,
  title={Neural data filter for bootstrapping stochastic gradient descent},
  author={Fan, Yang and Tian, Fei and Qin, Tao and Liu, Tie-Yan},
  year={2016}
}

@misc{isa,
  title = {国际自动化协会报告},
  howpublished = {\url{https://www.isa.org}},
}